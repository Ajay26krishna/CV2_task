{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1J2upH87AU5O-RbSmWQA9v7gLLZ6XeNSX","authorship_tag":"ABX9TyML8a8Oe7psGsbZY8yY4iWd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# compile/install code that is needed for this demo\n","# if it says that you need to restart the runtime, go ahead and do that,\n","# then run this step again to make sure the installs are complete\n","\n","!pip install git+https://github.com/nghorbani/human_body_prior\n","!pip install omegaconf\n","!pip install loguru\n","!pip install trimesh"],"metadata":{"id":"IdILGGoRBaIQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# connect to your google drive\n","# alternatively, upload the VPoserTest directory/files into Colab\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"stnWvb9eFkbK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"eogGTTZWLXic"}},{"cell_type":"code","source":["#filenames for loading VPoser VAE network, neutral SMPL body model, AMASS sample data\n","\n","from os import path as osp\n","\n","support_dir = '/content/gdrive/MyDrive/VPoserModelFiles/'\n","\n","expr_dir = osp.join(support_dir,'vposer_v2_05/') #'TRAINED_MODEL_DIRECTORY'\n","bm_fname =  osp.join(support_dir,'smplx_neutral_model.npz')    #'PATH_TO_SMPLX_model.npz'  neutral smpl body model\n","sample_amass_fname = osp.join(support_dir, 'amass_sample.npz')  # a sample npz file from AMASS\n","\n","\n","print(expr_dir)\n","print(bm_fname)\n","print(sample_amass_fname)"],"metadata":{"id":"vxer3_88ZNq9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Loading SMPLx Body Model\n","import torch\n","import numpy as np\n","\n","# Choose the device to run the body model on, cuda or cpu\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print('device is', device)\n","\n","from human_body_prior.body_model.body_model import BodyModel\n","bm = BodyModel(bm_fname=bm_fname).to(device)\n"],"metadata":{"id":"znBXfPLIZmXK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Loading VPoser VAE Body Pose Prior\n","from human_body_prior.tools.model_loader import load_model\n","from human_body_prior.models.vposer_model import VPoser\n","\n","vp, ps = load_model(expr_dir, model_code=VPoser,\n","                              remove_words_in_model_weights='vp_model.',\n","                              disable_grad=True,\n","                              comp_device=device)\n","vp = vp.to(device)"],"metadata":{"id":"E-96bbSCcBuz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare the body poses from amass sample file\n","#  indexing [3:66] removes global rotation, hands/fingers, and anything else other than 21 major body joints\n","amass_body_pose = np.load(sample_amass_fname)['poses'][:, 3:66]\n","amass_body_pose = torch.from_numpy(amass_body_pose).type(torch.float).to(device)\n","print('amass_body_pose.shape', amass_body_pose.shape)"],"metadata":{"id":"T1CVZk4Ef8zt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run the encoder on all frames\n","amass_body_poZ = vp.encode(amass_body_pose).mean\n","print('amass_body_poZ.shape', amass_body_poZ.shape)"],"metadata":{"id":"pwhSKj0XgJmy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run the decoder on all frames\n","amass_body_pose_rec = vp.decode(amass_body_poZ)['pose_body'].contiguous().view(-1, 63)\n","print('amass_body_pose_rec.shape', amass_body_pose_rec.shape)"],"metadata":{"id":"seEPQGOpgL4J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#get vertices and faces of a polygonal mesh model for each body pose\n","\n","from human_body_prior.tools.omni_tools import copy2cpu as c2c\n","import trimesh\n","\n","originalPoses = {'pose_body':amass_body_pose}\n","recoveredPoses = {'pose_body':amass_body_pose_rec}\n","\n","bmodelorig = bm(**originalPoses);\n","bmodelreco = bm(**recoveredPoses);\n","vorig = c2c(bmodelorig.v)\n","vreco = c2c(bmodelreco.v)\n","faces = c2c(bm.f)\n","\n","T, num_verts = vorig.shape[:-1]\n","\n"],"metadata":{"id":"ASdmx8xmzpQp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#visualize one frame's body pose before (grey) and after (purple) encode-decode\n","fIdx = 140\n","verts = vorig[fIdx]\n","mesh1 = trimesh.base.Trimesh(verts, faces)\n","mesh1.visual.vertex_colors = [254, 254, 254]\n","verts = vreco[fIdx]\n","mesh2 = trimesh.base.Trimesh(verts, faces)\n","mesh2.visual.vertex_colors = [254, 66, 200]\n","mesh2.apply_translation([1, 0, 0])  #use [0,0,0] to overlay them on each other\n","meshes = [mesh1, mesh2]\n","trimesh.Scene(meshes).show()"],"metadata":{"id":"V-Hqh4Fe3ja4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#visualize a temporal subsequence of poses spatially (use mouse to rotate view)\n","#note that encoding followed by decoding is not a lossless process,\n","#it can introduce a certain amount of error all by itself\n","\n","meshes = []\n","for fIdx in range(0,200,10):\n","    verts = vorig[fIdx]\n","    mesh1 = trimesh.base.Trimesh(verts, faces)\n","    mesh1.visual.vertex_colors = [254, 254, 254]\n","    mesh1.apply_translation([0, 0, fIdx*.07])\n","    meshes.append(mesh1)\n","    verts = vreco[fIdx]\n","    mesh1 = trimesh.base.Trimesh(verts, faces)\n","    mesh1.visual.vertex_colors = [254, 150, 200]\n","    mesh1.apply_translation([0, 0, fIdx*.07])\n","    meshes.append(mesh1)\n","\n","trimesh.Scene(meshes).show()"],"metadata":{"collapsed":true,"id":"WzyGrR8L-Gmh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extract and visualize 23 body joints before and after encode-decode process\n","# for a pose where error between original pose and decoded pose is rather large.\n","# why 23 instead of 21 mentioned earlier?  There are two extra joints somewhere\n","# that are not among the 21 rotatable body joints used by VPoser.\n","\n","fIdx = 130\n","\n","verts = vorig[fIdx]\n","mesh1 = trimesh.base.Trimesh(verts, faces)\n","mesh1.visual.vertex_colors = [254, 254, 254]\n","verts = vreco[fIdx]\n","mesh2 = trimesh.base.Trimesh(verts, faces)\n","mesh2.visual.vertex_colors = [254, 66, 200]\n","mesh2.apply_translation([0, 0, 0])  #use [0,0,0] to overlay them on each other\n","meshes = [mesh1, mesh2]\n","\n","\n","#get the 23 major 3D body joints\n","joints = c2c(bmodelorig.Jtr[fIdx])\n","origjoints = joints[0:23, :]   #ignore finger joints\n","joints = c2c(bmodelreco.Jtr[fIdx])\n","recojoints = joints[0:23, :]   #ignore finger joints\n","\n","print(origjoints.shape, recojoints.shape)\n","for i in range(origjoints.shape[0]):\n","    sphere = trimesh.primitives.Sphere(radius=.02, center=origjoints[i,:])\n","    sphere.apply_translation([1, 0, 0])\n","    sphere.visual.vertex_colors = [254, 254, 254]\n","    meshes.append(sphere)\n","    sphere = trimesh.primitives.Sphere(radius=.02, center=recojoints[i,:])\n","    sphere.apply_translation([1, 0, 0])\n","    sphere.visual.vertex_colors = [254, 150, 200]\n","    meshes.append(sphere)\n","\n","trimesh.Scene(meshes).show()\n","\n"],"metadata":{"id":"i3ztwAADQmjg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Y94UuuQYEI9f"},"execution_count":null,"outputs":[]}]}